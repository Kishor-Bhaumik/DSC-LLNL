{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weights & Biases (W&B) Logging Tutorial for PyTorch\n",
    "\n",
    "This tutorial demonstrates how to use Weights & Biases (W&B) for experiment tracking and visualization, first with a simple loop, and then with a basic PyTorch neural network for sine wave approximation.\n",
    "\n",
    "W&B helps you track:\n",
    "* **Metrics:** Loss, accuracy, F1 score, etc.\n",
    "* **Hyperparameters:** Learning rate, batch size, optimizer, etc.\n",
    "* **Model Artifacts:** Save model checkpoints.\n",
    "* **System Metrics:** CPU/GPU utilization, memory usage.\n",
    "* **Media:** Images, videos, audio, custom plots.\n",
    "\n",
    "To get started, you'll need to install `wandb`:\n",
    "```bash\n",
    "pip install wandb torch torchvision torchaudio\n",
    "```\n",
    "You will also need to log in to your W&B account. Run `wandb login` in your terminal or uncomment and run `wandb.login()` in a code cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Basic Logging with a For Loop\n",
    "\n",
    "In this section, we'll demonstrate the fundamental `wandb.init()` and `wandb.log()` functions. \n",
    "\n",
    "**`wandb.init()`**: This function initializes a new W&B run. A 'run' is a single execution of your code, typically corresponding to one experiment. You can specify a `project` name to group related runs and a `name` for the specific run to easily identify it later. If you don't specify these, W&B will generate default names.\n",
    "\n",
    "**`wandb.log()`**: This function is used to log data to the current W&B run. It takes a dictionary where keys are the names of the metrics/data you want to log (e.g., `'loss'`, `'accuracy'`) and values are their corresponding numerical values. W&B automatically plots these values over time (steps or epochs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import random\n",
    "\n",
    "# Optional: Uncomment the line below to log in if you haven't already\n",
    "# wandb.login()\n",
    "\n",
    "# 1. Initialize a new W&B run\n",
    "# We give it a project name and a specific run name for easy identification.\n",
    "wandb.init(project=\"simple-logging-tutorial\", name=\"for-loop-example\")\n",
    "\n",
    "print(\"Starting basic logging example...\")\n",
    "\n",
    "# Simulate a training process with a simple for loop\n",
    "for i in range(100):\n",
    "    # Simulate a decreasing loss and increasing accuracy\n",
    "    simulated_loss = 100 / (i + 1) + random.uniform(-0.5, 0.5)\n",
    "    simulated_accuracy = 0.5 + (i / 100) * 0.4 + random.uniform(-0.02, 0.02)\n",
    "\n",
    "    # 2. Log metrics to W&B\n",
    "    # The dictionary keys will be the names of your plots in the W&B UI.\n",
    "    wandb.log({\"simulated_loss\": simulated_loss, \"simulated_accuracy\": simulated_accuracy})\n",
    "\n",
    "    if i % 20 == 0:\n",
    "        print(f\"Step {i}: Loss = {simulated_loss:.2f}, Accuracy = {simulated_accuracy:.2f}\")\n",
    "\n",
    "print(\"Basic logging example finished. Check your W&B dashboard!\")\n",
    "\n",
    "# Finish the W&B run\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Sine Wave Approximation with PyTorch and W&B\n",
    "\n",
    "Now, let's apply W&B logging to a more realistic scenario: training a simple neural network to approximate a sine wave. We'll generate synthetic data, define a small PyTorch model, and then train it while logging training, validation, and test performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- 1. Data Generation ---\n",
    "print(\"Generating sine wave data...\")\n",
    "\n",
    "# Generate x values\n",
    "X = np.linspace(-2 * np.pi, 2 * np.pi, 1000).reshape(-1, 1).astype(np.float32)\n",
    "# Generate y values (sine wave with some noise)\n",
    "y = np.sin(X) + np.random.normal(0, 0.1, X.shape).astype(np.float32)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.from_numpy(X)\n",
    "y_tensor = torch.from_numpy(y)\n",
    "\n",
    "# --- 2. Data Splitting ---\n",
    "# Split into training (70%), validation (15%), and test (15%) sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_tensor, y_tensor, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Train samples: {len(X_train)}\")\n",
    "print(f\"Validation samples: {len(X_val)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")\n",
    "\n",
    "# Optional: Visualize the data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_train.numpy(), y_train.numpy(), label='Train Data', s=5, alpha=0.6)\n",
    "plt.scatter(X_val.numpy(), y_val.numpy(), label='Validation Data', s=5, alpha=0.6)\n",
    "plt.scatter(X_test.numpy(), y_test.numpy(), label='Test Data', s=5, alpha=0.6)\n",
    "plt.title('Sine Wave Data for Approximation')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# --- 3. Define a Simple Neural Network ---\n",
    "print(\"Defining the neural network...\")\n",
    "\n",
    "class SineApproximator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SineApproximator, self).__init__()\n",
    "        # A very simple network with two hidden layers\n",
    "        self.fc1 = nn.Linear(1, 64)  # Input: 1 feature (X), Output: 64 features\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(64, 64) # Input: 64 features, Output: 64 features\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(64, 1)  # Input: 64 features, Output: 1 feature (Y)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = SineApproximator()\n",
    "print(\"Model architecture:\")\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import wandb\n",
    "\n",
    "# Ensure previous W&B run is finished if any\n",
    "if wandb.run is not None:\n",
    "    wandb.finish()\n",
    "\n",
    "# --- 4. Training, Validation, and Testing with W&B Logging ---\n",
    "print(\"Starting training with W&B logging...\")\n",
    "\n",
    "# Define hyperparameters\n",
    "config = {\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"epochs\": 500,\n",
    "    \"batch_size\": 32, # Not strictly used for full batch, but good to define\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"loss_function\": \"MSELoss\"\n",
    "}\n",
    "\n",
    "# Initialize W&B run for this training experiment\n",
    "wandb.init(\n",
    "    project=\"pytorch-sine-approximation\",  # Group related runs\n",
    "    name=\"simple-sine-model-run\",          # Specific name for this run\n",
    "    config=config                          # Log hyperparameters\n",
    ")\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss() # Mean Squared Error is good for regression tasks\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=wandb.config.learning_rate)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(wandb.config.epochs):\n",
    "    # --- Training Step ---\n",
    "    model.train() # Set model to training mode\n",
    "    optimizer.zero_grad() # Clear gradients\n",
    "    outputs = model(X_train) # Forward pass\n",
    "    train_loss = criterion(outputs, y_train) # Calculate loss\n",
    "    train_loss.backward() # Backward pass (calculate gradients)\n",
    "    optimizer.step() # Update weights\n",
    "\n",
    "    # --- Validation Step ---\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    with torch.no_grad(): # Disable gradient calculation for validation\n",
    "        val_outputs = model(X_val)\n",
    "        val_loss = criterion(val_outputs, y_val)\n",
    "\n",
    "    # Log metrics to W&B\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch,\n",
    "        \"train_loss\": train_loss.item(),\n",
    "        \"val_loss\": val_loss.item()\n",
    "    })\n",
    "\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{wandb.config.epochs}], Train Loss: {train_loss.item():.4f}, Val Loss: {val_loss.item():.4f}')\n",
    "\n",
    "print(\"Training finished.\")\n",
    "\n",
    "# --- 5. Final Test Performance ---\n",
    "print(\"Evaluating on test set...\")\n",
    "model.eval() # Set model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(X_test)\n",
    "    test_loss = criterion(test_outputs, y_test)\n",
    "\n",
    "print(f'Final Test Loss: {test_loss.item():.4f}')\n",
    "\n",
    "# Log final test loss to W&B\n",
    "wandb.log({\"final_test_loss\": test_loss.item()})\n",
    "\n",
    "# Optional: Plot the model's predictions against the actual data\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predicted_y = model(X_tensor.to(device)).cpu().numpy()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_tensor.numpy(), y_tensor.numpy(), label='True Data', s=5, alpha=0.6)\n",
    "plt.plot(X_tensor.numpy(), predicted_y, color='red', label='Model Predictions')\n",
    "plt.title('Sine Wave Approximation: Model Predictions vs. True Data')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Finish the W&B run\n",
    "wandb.finish()\n",
    "\n",
    "print(\"Tutorial complete. Check your W&B dashboard for detailed logs!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
